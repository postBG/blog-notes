# 4.0 Entropy

## Entropy (= 정보량)
정보를 최적으로 압축시키기 위해 필요한 비트 수

## 어떻게 정보를 최적으로 압축시킬까?
1. 만약에 같은 확률로 발생하는 것이라면 그냥 log 2를 취하면 된다.
    * ex) 요일을 표현하려면 => 3 bits면 충분
2. 만약에 발생하는 확률이 서로 다르면 어떻게 하면 좋을까?
    * ex) 40개의 서로 다른 문자를 전송하는데, A, B, C, D 4개의 문자가 각각 22.5%씩 전체의 90% 확률로 발생한다.
    * 그렇다면 자주 발생하는 일들은 적은 비트로 표현하고, 예외적으로 발생하는 10%는 더 많은 비트를 사용하는게 글로벌하게 보면 더 이득이다.
    * 예를 들어 그냥 naive하게 40개를 표현하면 5.3bits가 필요 (log_2 40)
    * 하지만 1 bit로 ABCD인지 아닌지를 구별하고, ABCD인 경우 2bits로 아닌 경우 6bits로 표현하면,
    * 평균적으로 0.9 * (3) + 0.1 * (7) = 3.4 bits가 필요
3. 결국, Entropy는 각각의 label의 확률 분포와 연관된다.
    * H(y) = Sigma(y_i * log(1 / y_i)) = - Sigma(y_i * log(y_i))
  
