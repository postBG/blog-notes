# 4.1 Cross Entropy & KLD

4.0에서 말했듯이 Entropy는 확률의 분포를 고려하여 인코딩하는데 필요한 비트수. 
수식으로는 아래와 같이 표현된다.
```python
// ys = 각 label들의 발생 확률을 담은 것, sum(ys) = 1
entropy = sum([y * log(1 / y) for y in ys])

# log(1 / y)가 information gain인 이유는, 발생확률이 높을수록 흔하므로 얻을 수 있는 정보량이 적기 때문
# 즉, 희귀성이 곧 information gain이고 이는 발생확률에 반비례
information_gains = [log(1 / y) for y in ys] 
```

## Cross Entropy
우리가 만약에 정답 분포가 있는 상태(이 정답 분포를 **p**라고 하자)에서 잘못된 정보를 가지고 있는 상태(이런 잘못된 확률 분포를 **q**라고 하자.)에서 Entropy를 계산했다고 해보자.
그러면 실제의 Entropy와는 값이 다를 것인데, 그 식은 아래와 같다.
```python
// ps = 정답 확률들이 들어있다고 하자, qs = 잘못된 확률들이 들어있다고 하자
// labels = 데이터들이 들어있다고 하자
cross_entropy = sum([ps[label] * log(1 / qs[label]) for label in labels])
```

## KL-Divergence
최적의 인코딩과 최적이 아닌 분포의 인코딩의 차이 (P가 실제 데이터의 분포이고, Q가 우리가 예측한 분포라고 가정하자)
`KLD = H{P, Q) - H(P) = (cross entropy between P and Q) - (entropy of P)`
그러면 KLD를 minimize하는 것은 곧 cross entropy를 minimize하는 것과 동일하다.
왜냐하면, H(P)는 머신러닝에서는 그냥 상수이기 때문.

## Classification에서 MSE보다 Cross Entropy를 많이 쓰는 이유
MSE를 미분해서 gradient를 구해보면, y_hat^2에 비례하게 되서 sigmoid같이 쉽게 값이 saturate되면 gradient가 거의 0이 되는 경우가 많지만,
Cross Entropy로 gradient를 구해보면 (y_hat - y)에 비례하게 되서 좋다.
